{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "import pandas as pd  \n",
    "\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matlab.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ran=1;%for GBM\n",
    "#ran=5;%for KNN\n",
    "#Please manually change in Matlab code reading_hotspot2Data_v3\n",
    "os.getcwd()\n",
    "os.chdir('/Users/sitani/Desktop/CS_final_codes/workonhotspots/')\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.reading_hotspot2Data_v3(nargout=0)\n",
    "os.chdir('/Users/sitani/Desktop/CS_final_codes/workonhotspots/FinalCodesForPublication/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv('Atrain_whole.csv',header=None)\n",
    "y=pd.read_csv('label_whole.csv',header=None)\n",
    "\n",
    "X_train = pd.read_csv('Atrain2.csv',header=None)\n",
    "X_valid = pd.read_csv('Avalid2.csv',header=None)\n",
    "\n",
    "y_train=pd.read_csv('label_train2.csv',header=None)\n",
    "y_valid=pd.read_csv('label_valid2.csv',header=None)\n",
    "\n",
    "X_test=pd.read_csv('Atest2.csv',header=None)\n",
    "y_test=pd.read_csv('label_test2.csv',header=None)\n",
    "\n",
    "X_train=X_train.T#transpose\n",
    "X_valid=X_valid.T\n",
    "X=X.T\n",
    "X_test=X_test.T\n",
    "\n",
    "y_train=y_train.values.ravel()\n",
    "y_valid=y_valid.values.ravel()\n",
    "y=y.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter values that should be searched\n",
    "k_range = list(range(1, 40))\n",
    "\n",
    "# Another parameter besides k that we might vary is the weights parameters\n",
    "# default options --> uniform (all points in the neighborhood are weighted equally)\n",
    "# another option --> distance (weights closer neighbors more heavily than further neighbors)\n",
    "\n",
    "# we create a list\n",
    "weight_options = ['uniform', 'distance']\n",
    "#weight_options = ['uniform']\n",
    "score=['accuracy','f1','precision','recall','roc_auc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "# dictionary = dict(key=values, key=values)\n",
    "param_grid = dict(n_neighbors=k_range, weights=weight_options)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and fit the grid\n",
    "# exhaustive grid-search because it's trying every combination\n",
    "# 10-fold cross-validation is being performed 30 x 2 = 60 times\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "grid = GridSearchCV(knn, param_grid, cv=5,scoring='recall')\n",
    "grid.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the complete results\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_,\n",
    "#grid.grid_scores_(min(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model using all data and the best known parameters\n",
    "param1=grid.best_params_[\"n_neighbors\"]\n",
    "param2=grid.best_params_[\"weights\"]\n",
    "# instantiate model with best parameters\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=param1, weights=param2)\n",
    "\n",
    "# fit with X and y, not X_train and y_train\n",
    "# even if we use train/valid split, we should train on X and y before making predictions on new data\n",
    "# otherwise we throw away potential valuable data we can learn from\n",
    "knn.fit(X, y)\n",
    "\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "#grid.best_params_[\"n_neighbors\"]\n",
    "# Best score did not improve for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "#grid.best_params_[\"n_neighbors\"]\n",
    "# Best score did not improve for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=param1, weights=param2)\n",
    "\n",
    "knn.fit(X, y)\n",
    "\n",
    "Acc=cross_val_score(knn, X, y, scoring='accuracy',\n",
    "          cv=5)\n",
    " \n",
    "print('Accuracy: %.3f' % Acc.mean() )\n",
    "\n",
    "Sen=cross_val_score(knn, X, y, scoring='recall',\n",
    "          cv=5)\n",
    "Sen.mean()  \n",
    "print('Sensitivity: %.3f' % Sen.mean() )\n",
    "Prec=cross_val_score(knn, X, y, scoring='precision',\n",
    "          cv=5)\n",
    "Prec.mean()  \n",
    "print('Prec: %.3f' % Prec.mean() )\n",
    "F1=cross_val_score(knn, X, y, scoring='f1',\n",
    "          cv=5)\n",
    "F1.mean()  \n",
    "print('F1: %.3f' % F1.mean() )\n",
    "AUC=cross_val_score(knn, X, y, scoring='roc_auc',\n",
    "          cv=5)\n",
    "AUC.mean()  \n",
    "print('AUC: %.3f' % AUC.mean() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "import math\n",
    "\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "\n",
    "scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn),\n",
    "           'fp': make_scorer(fp), 'fn': make_scorer(fn)}\n",
    "cv_results = cross_validate(knn.fit(X, y), X, y,\n",
    "                           scoring=scoring, cv=5)\n",
    "tp=cv_results['test_tp'].mean()\n",
    "fp=cv_results['test_fp'].mean()\n",
    "tn=cv_results['test_tn'].mean()\n",
    "fn=cv_results['test_fn'].mean()\n",
    "print('tp: %.3f' % tp)\n",
    "print('\\n')\n",
    "print('tn: %.3f' % tn)\n",
    "print('\\n')\n",
    "print('fp: %.3f' % fp)\n",
    "print('\\n')\n",
    "print('fn: %.3f' % fn)\n",
    "print('\\n')\n",
    "\n",
    "specificity=tn/(tn+fp)\n",
    "print('Specificity: %.3f' %specificity)\n",
    "\n",
    "MCC= (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "print('MCC:%.3f' %MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "#knn.fit(X,y)\n",
    "knn = KNeighborsClassifier(n_neighbors=param1, weights=param2)\n",
    "\n",
    "\n",
    "\n",
    "knn.fit(X, y)\n",
    "y_test_pred = knn.predict(X_test) \n",
    "knn_roc_auc = roc_auc_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# predict probabilities\n",
    "\n",
    "probs = knn.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='KNN(area = %0.2f)' % knn_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.ylabel('True Positive Rate(Sensitivity)')\n",
    "plt.xlabel('False Positive Rate(1-Specificity)')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('KNN_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "print('Accuracy of KNN classifier: {:.2f}'.format(knn.score(X_test, y_test)))\n",
    "print('\\n')\n",
    "confusion_df = pd.DataFrame(confusion_matrix(y_test,y_test_pred),\n",
    "             columns=[\"Predicted Class \" + str(class_name) for class_name in [1,0]],\n",
    "             index = [\"Class \" + str(class_name) for class_name in [1,0]])\n",
    "\n",
    "print('Confusion matrix : \\n',confusion_df.T)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "cm1 = confusion_matrix(y_test, y_test_pred).T\n",
    "print('\\n')\n",
    "total1=sum(sum(cm1))\n",
    "print(cm1)\n",
    "#####from confusion matrix calculate sensitivity specificity\n",
    "\n",
    "tn=confusion_matrix(y_test, y_test_pred)[0, 0]\n",
    "fp=confusion_matrix(y_test, y_test_pred)[0, 1]\n",
    "fn=confusion_matrix(y_test, y_test_pred)[1, 0]\n",
    "tp=confusion_matrix(y_test, y_test_pred)[1, 1]\n",
    "#sensitivity1=tp/(tp+fn)\n",
    "\n",
    "#print('Sensitivity :', sensitivity1 )\n",
    "#print('\\n')\n",
    "\n",
    "specificity1=tn/(tn+fp)\n",
    "print('Specificity :', specificity1)\n",
    "print('\\n')\n",
    "#precision=tp/(tp+fp)\n",
    "#precision = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "#print('precision :', precision)\n",
    "#print('\\n')\n",
    "from sklearn.metrics import precision_score\n",
    "print(\"Precision score: {}\".format(precision_score(y_test,y_test_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"Recall or Sensitivity: {}\".format(recall_score(y_test,y_test_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test,y_test_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "print(\"Matthews Correlation Coefficient: {}\".format(matthews_corrcoef(y_test, y_test_pred)))\n",
    "print('\\n')\n",
    "##Area under Curve-AUC\n",
    "auc = roc_auc_score(y_test, knn.predict_proba(X_test)[:,1])\n",
    "print('AUC: %.3f' % auc)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
