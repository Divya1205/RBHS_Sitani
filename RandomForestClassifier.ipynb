{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = (16, 8)\n",
    "#print(__doc__)\n",
    "import matlab.engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/Users/sitani/Desktop/CS_final_codes/workonhotspots/')\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.reading_hotspot2Data_v4(nargout=0)\n",
    "os.chdir('/Users/sitani/Desktop/CS_final_codes/workonhotspots/FinalCodesForPublication/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv('Atrain_whole.csv',header=None)\n",
    "y=pd.read_csv('label_whole.csv',header=None)\n",
    "\n",
    "X_train = pd.read_csv('Atrain2.csv',header=None)\n",
    "#X_valid = pd.read_csv('Avalid2.csv',header=None)\n",
    "\n",
    "y_train=pd.read_csv('label_train2.csv',header=None)\n",
    "#y_valid=pd.read_csv('label_valid2.csv',header=None)\n",
    "\n",
    "X_test=pd.read_csv('Atest2.csv',header=None)\n",
    "y_test=pd.read_csv('label_test2.csv',header=None)\n",
    "\n",
    "X_train=X_train.T#transpose\n",
    "#X_valid=X_valid.T\n",
    "X=X.T\n",
    "X_test=X_test.T\n",
    "\n",
    "y_train=y_train.values.ravel()\n",
    "#y_valid=y_valid.values.ravel()\n",
    "y=y.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "print(X_train.shape)\n",
    "#print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "#print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [3,5,8,13,20,25,30,35,40],\n",
    "    'max_features': [2,7,14,34],\n",
    "    'min_samples_leaf': [2,3,4,5,10,15,20,26],\n",
    "    'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12],\n",
    "    'n_estimators': [100, 150,200,250, 300,350 ]\n",
    "}\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "#bootstrap = [True, False]\n",
    "bootstrap = [True]\n",
    "# Create the random grid\n",
    "param_grid_old = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5,n_jobs = -1,scoring='accuracy', verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(bootstrap=grid_search.best_params_[\"bootstrap\"], class_weight=None, criterion='gini',\n",
    "            max_depth=grid_search.best_params_[\"max_depth\"], \n",
    "            max_features=grid_search.best_params_[\"max_features\"], max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=grid_search.best_params_[\"min_samples_leaf\"], \n",
    "            min_samples_split=grid_search.best_params_[\"min_samples_split\"],\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=grid_search.best_params_[\"n_estimators\"],\n",
    "            oob_score=False, random_state=0, verbose=0, warm_start=False)  \n",
    "\n",
    "classifier.fit(X, y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "Acc=cross_val_score(classifier, X, y, scoring='accuracy',\n",
    "          cv=5)\n",
    " \n",
    "print('Accuracy: %.3f' % Acc.mean() )\n",
    "\n",
    "Sen=cross_val_score(classifier, X, y, scoring='recall',\n",
    "          cv=5)\n",
    "Sen.mean()  \n",
    "print('Sensitivity: %.3f' % Sen.mean() )\n",
    "Prec=cross_val_score(classifier, X, y, scoring='precision',\n",
    "          cv=5)\n",
    "Prec.mean()  \n",
    "print('Prec: %.3f' % Prec.mean() )\n",
    "F1=cross_val_score(classifier, X, y, scoring='f1',\n",
    "          cv=5)\n",
    "F1.mean()  \n",
    "print('F1: %.3f' % F1.mean() )\n",
    "AUC=cross_val_score(classifier, X, y, scoring='roc_auc',\n",
    "          cv=5)\n",
    "AUC.mean()  \n",
    "print('AUC: %.3f' % AUC.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "import math\n",
    "\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "\n",
    "scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn),\n",
    "           'fp': make_scorer(fp), 'fn': make_scorer(fn)}\n",
    "cv_results = cross_validate(classifier.fit(X, y), X, y,\n",
    "                           scoring=scoring, cv=5)\n",
    "tp=cv_results['test_tp'].mean()\n",
    "fp=cv_results['test_fp'].mean()\n",
    "tn=cv_results['test_tn'].mean()\n",
    "fn=cv_results['test_fn'].mean()\n",
    "print('tp: %.3f' % tp)\n",
    "print('\\n')\n",
    "print('tn: %.3f' % tn)\n",
    "print('\\n')\n",
    "print('fp: %.3f' % fp)\n",
    "print('\\n')\n",
    "print('fn: %.3f' % fn)\n",
    "print('\\n')\n",
    "\n",
    "specificity=tn/(tn+fp)\n",
    "print('Specificity: %.3f' %specificity)\n",
    "\n",
    "MCC= (tp*tn-fp*fn)/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "print('MCC:%.3f' %MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "y_test_pred = classifier.predict(X_test) \n",
    "RF_roc_auc = roc_auc_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "RF_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "# predict probabilities\n",
    "\n",
    "probs = classifier.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='RF(area = %0.2f)' % RF_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.ylabel('True Positive Rate(Sensitivity)')\n",
    "plt.xlabel('False Positive Rate(1-Specificity)')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('RF_ROC')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "print('Accuracy of RF classifier: {:.2f}'.format(classifier.score(X_test, y_test)))\n",
    "print('\\n')\n",
    "confusion_df = pd.DataFrame(confusion_matrix(y_test,y_test_pred),\n",
    "             columns=[\"Predicted Class \" + str(class_name) for class_name in [1,0]],\n",
    "             index = [\"Class \" + str(class_name) for class_name in [1,0]])\n",
    "\n",
    "print('Confusion matrix : \\n',confusion_df.T)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "cm1 = confusion_matrix(y_test, y_test_pred).T\n",
    "print('\\n')\n",
    "total1=sum(sum(cm1))\n",
    "print(cm1)\n",
    "#####from confusion matrix calculate sensitivity specificity\n",
    "\n",
    "tn=confusion_matrix(y_test, y_test_pred)[0, 0]\n",
    "fp=confusion_matrix(y_test, y_test_pred)[0, 1]\n",
    "fn=confusion_matrix(y_test, y_test_pred)[1, 0]\n",
    "tp=confusion_matrix(y_test, y_test_pred)[1, 1]\n",
    "#sensitivity1=tp/(tp+fn)\n",
    "\n",
    "#print('Sensitivity :', sensitivity1 )\n",
    "#print('\\n')\n",
    "\n",
    "specificity1=tn/(tn+fp)\n",
    "print('Specificity :', specificity1)\n",
    "print('\\n')\n",
    "#precision=tp/(tp+fp)\n",
    "#precision = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "#print('precision :', precision)\n",
    "#print('\\n')\n",
    "from sklearn.metrics import precision_score\n",
    "print(\"Precision score: {}\".format(precision_score(y_test,y_test_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"Recall or Sensitivity: {}\".format(recall_score(y_test,y_test_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test,y_test_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "print(\"Matthews Correlation Coefficient: {}\".format(matthews_corrcoef(y_test, y_test_pred)))\n",
    "print('\\n')\n",
    "##Area under Curve-AUC\n",
    "auc = roc_auc_score(y_test, classifier.predict_proba(X_test)[:,1])\n",
    "print('AUC: %.3f' % auc)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_matrix(y_test,final_pred),\n",
    "             columns=[\"Predicted Class \" + str(class_name) for class_name in [0,1]],\n",
    "             index = [\"Class \" + str(class_name) for class_name in [0,1]])\n",
    "\n",
    "print('Confusion matrix : \\n',confusion_df)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "cm1 = confusion_matrix(y_test, final_pred)\n",
    "print('\\n')\n",
    "total1=sum(sum(cm1))\n",
    "\n",
    "#####from confusion matrix calculate sensitivity specificity\n",
    "\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print('Accuracy :', accuracy1)\n",
    "print('\\n')\n",
    "sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Sensitivity :', sensitivity1 )\n",
    "print('\\n')\n",
    "specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Specificity :', specificity1)\n",
    "print('\\n')\n",
    "from sklearn.metrics import precision_score\n",
    "print(\"Precision score: {}\".format(precision_score(y_test,final_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test,final_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "print(\"Matthews Correlation Coefficient: {}\".format(matthews_corrcoef(y_test, final_pred)))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract single tree\n",
    "estimator = classifier.estimators_[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, \n",
    "                out_file='tree.dot', \n",
    "                \n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
